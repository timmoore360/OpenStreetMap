{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap Project\n",
    "# Data Wrangling with MongoDB\n",
    "### Tim Moore\n",
    "\n",
    "Map Area: Seattle, WA, United States\n",
    "\n",
    "https://mapzen.com/data/metro-extracts/#seattle-washington\n",
    "\n",
    "http://www.openstreetmap.org/relation/237385"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problems Encountered in the Map\n",
    "\n",
    "The Seattle .xml file downloaded from mapzen.com turned out to be quite large at 1.46 GB once it was unzipped. I utilized the provided code from the project details page to work on 1/10th of it initially. Once I had my wrangling helper functions, file processing function and database upload working correctly on a smaller scale, I began running my code using the full dataset. \n",
    "\n",
    "A few of the larger challenges I had were:\n",
    "- Standardizing postal codes (the Seattle map has some overlap with the Victoria B.C. Map, and Canadian postal codes are in a different format)\n",
    "- Cleaning up and standardizing city names (capitalizing consistently, removing state info if provided, handling a few one-off cases)\n",
    "- Cleaning up and standardizing street names\n",
    "\n",
    "\n",
    "### Standardizing postal codes\n",
    "\n",
    "My map data included parts of Canada in the Seattle data, this caused issues mostly with post codes. Canadian post codes are in a format of A1A A1A though many users combined the two groups together. This allowed me to become more of an expert with the re module of python as I built the function to standardize all postcodes before they were written to the JSON file that was passed into MongoDB. \n",
    "\n",
    "I decided to update all Canadian post codes into XXX XXX format and truncated US post codes down to only 5 digits. I decided to return the None type for any input that wasn't correct but I was unable to correct it due to included data. \n",
    "\n",
    "\n",
    "### Cleaning up and standardizing City Names\n",
    "\n",
    "I found some city names that included other identifying features, for example 'Oak Harbor (whidbey island)'. I decided that additional levels of identification were ok here and changed this example into 'Oak Harbor Whidbey Island'. I also removed any state/province information that may have been included by the creator, and made sure every city name was capitalized.\n",
    "\n",
    "\n",
    "### Cleaning up and standardizing Street Names\n",
    "\n",
    "Most of the street names that needed to be cleaned up turned out to be Directional street names (NE 116th st becomes Northeast 116th Street). I updated the strings prior to saving as a JSON file to limit the number of times I had to open and close files due to the size of this map.\n",
    "\n",
    "# Helper functions and Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "import codecs\n",
    "import json\n",
    "import sys\n",
    "sys.path.append('c:/python27/lib/site-packages')\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "OSM_FILE = \"c:/users/moore/documents/udacity/project 3/seattle_washington.osm/seattle_washington.osm\"\n",
    "\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", 'Northeast', 'Northwest', 'Southwest', 'Southeast', 'East',\n",
    "           'West', 'South', 'North', 'Terrace']\n",
    "\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Ave.\": \"Avenue\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Blvd.\": \"Boulevard\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"Dr.\": \"Drive\",\n",
    "            \"Ct\": \"Court\",\n",
    "            \"Ct.\": \"Court\",\n",
    "            \"Pl\": \"Place\",\n",
    "            \"Pl.\": \"Place\",\n",
    "            \"Sq\": \"Square\",\n",
    "            \"Sq.\": \"Square\",\n",
    "            \"Ln\": \"Lane\",\n",
    "            \"Ln.\": \"Lane\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Rd\" : \"Road\",\n",
    "            \"Tr\" : \"Trail\",\n",
    "            \"Tr.\": \"Trail\",\n",
    "            \"Pkwy\": \"Parkway\",\n",
    "            \"Pkwy.\": \"Parkway\",\n",
    "            \"NE\": \"Northeast\",\n",
    "           'NW': 'Northwest',\n",
    "           'SW': 'Southwest',\n",
    "           'SE': 'Southeast',\n",
    "           'E': 'East',\n",
    "           'W': 'West',\n",
    "           'S': 'South',\n",
    "           'N': 'North',\n",
    "           'Ter': 'Terrace'           \n",
    "            }\n",
    "## Helper functions\n",
    "\n",
    "def aggregate(db, pipeline):\n",
    "    return [doc for doc in seattle.aggregate(pipeline)]\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    m = street_type_re.search(name)\n",
    "    other_street_types = []\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type in mapping.keys():\n",
    "            name = re.sub(street_type, mapping[street_type],name)\n",
    "        else:\n",
    "            other_street_types.append(street_type)\n",
    "    return name\n",
    "\n",
    "def update_postcode(postcode):\n",
    "    if re.search(r'\\d{5}', postcode):\n",
    "        postcode = re.search(r'\\d{5}', postcode).group()\n",
    "    elif re.search(r'\\d{5}(?:[-\\s]\\d{4})?$', postcode):\n",
    "        postcode = re.search( r'^\\d{5}',postcode).group()\n",
    "    elif re.search(r'\\w{6}', postcode):\n",
    "        firstpart, secondpart = postcode[:len(postcode)/2], postcode[len(postcode)/2:]\n",
    "        postcode = firstpart + ' ' + secondpart\n",
    "    elif re.search(r'(\\w{3})\\s(\\w{3})', postcode):\n",
    "        postcode = re.search(r'\\w{3}\\s(\\w{3})', postcode).group()\n",
    "    elif re.search(r'[WA]', postcode):\n",
    "        postcode = None\n",
    "    else:\n",
    "        return postcode\n",
    "    return postcode   \n",
    "\n",
    "def update_city(city):\n",
    "    try:\n",
    "        correct_name = city.split(',')\n",
    "        if len(city.split(';')) == 2:\n",
    "            correct_name = city.split(';')[0]\n",
    "        name = ''\n",
    "        for subname in correct_name[0].split():\n",
    "            name += subname.strip('()').capitalize() + ' '\n",
    "        return name.rstrip(' ')\n",
    "    except:\n",
    "        return city.capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing .xml file and shaping it into the desired JSON structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Processing each element from the .xml dataset and manipulating it into our desired JSON shape\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "GENERAL = [ 'visible', 'amenity', 'cuisine', 'name', 'phone']\n",
    "\n",
    "def shape_element(element):\n",
    "    node = {}\n",
    "    node['created'] = {}\n",
    "    address = {}\n",
    "    node['pos'] = []\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        for x in CREATED:\n",
    "            if x in element.attrib:\n",
    "                node['created'][x] = element.attrib[x]\n",
    "        node['id'] = element.attrib['id']\n",
    "        for x in GENERAL:\n",
    "            if x in element.attrib:\n",
    "                node[x] = element.attrib[x]\n",
    "        node['type'] = element.tag\n",
    "        try:\n",
    "            node['pos'].append(float(element.attrib['lat']))\n",
    "            node['pos'].append(float(element.attrib['lon']))\n",
    "        except: \n",
    "            pass\n",
    "        node_refs = []\n",
    "        for subtag in element:\n",
    "            if subtag.tag == 'tag':\n",
    "                if re.search(problemchars, subtag.get('k')):\n",
    "                    pass\n",
    "                elif re.search(r'\\w+:\\w+:\\w+', subtag.get('k')):\n",
    "                    pass\n",
    "                elif subtag.get('k').startswith('addr:'):\n",
    "                    if re.search(r'addr:street', subtag.get('k')):\n",
    "                        address[\"street\"]=update_name(subtag.attrib['v'],mapping)             \n",
    "                    \n",
    "                    elif re.findall('post',subtag.get('k')):\n",
    "                        address[\"postcode\"]=update_postcode(subtag.attrib[\"v\"])\n",
    "                        # This takes care of a single issue with user input\n",
    "                        if address['postcode'] == 'W Lake Samma mish Pkwy NE':\n",
    "                            address['postcode'] = None\n",
    "                            address['street'] = 'W Lake Sammamish Pkwy NE'\n",
    "                    elif re.findall('city',subtag.get('k')):\n",
    "                        address[\"city\"]=update_city(subtag.attrib['v'])\n",
    "                    else:\n",
    "                        address[subtag.get('k')[5:]] = subtag.get('v')\n",
    "                else:\n",
    "                    node[subtag.get('k')] = subtag.get('v')\n",
    "            else:\n",
    "                if subtag.tag == 'nd':\n",
    "                    node_refs.append(subtag.get('ref'))        \n",
    "                else:\n",
    "                    pass\n",
    "        if node_refs:\n",
    "            node['node_refs'] = node_refs\n",
    "        if address:\n",
    "            node['address'] = address\n",
    "        \n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def process_map(file_in, pretty = False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing .xml file and inserting it to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = process_map(OSM_FILE, False)\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "client.drop_database('seattle')\n",
    "db = client.seattle\n",
    "seattle = db.seattle\n",
    "result = seattle.insert_many(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Overview\n",
    "This section contains basic statisticcs about the dataset I investigated as well as the MongoDB queries used to gather them.\n",
    "\n",
    "### File size\n",
    "seattle_washington.osm     ...........       1.46 GB\n",
    "\n",
    "seattle_washington.osm.json   ...    1.66 GB\n",
    "\n",
    "### Total number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7711454"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seattle.find().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7034903"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seattle.find({'type': 'node'}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676450"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seattle.find({'type': 'way'}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top contributing user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'Glassman', u'count': 1237556}]\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{'$group': {'_id': '$created.user', 'count' : {'$sum': 1}}},\n",
    "            {'$sort': {'count':-1}}, \n",
    "            {\"$limit\":1}]\n",
    "top_user = aggregate(seattle, pipeline)\n",
    "pprint.pprint(top_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'Glassman', u'count': 1237556},\n",
      " {u'_id': u'SeattleImport', u'count': 745875},\n",
      " {u'_id': u'tylerritchie', u'count': 661401},\n",
      " {u'_id': u'woodpeck_fixbot', u'count': 615971},\n",
      " {u'_id': u'alester', u'count': 329568},\n",
      " {u'_id': u'Glassman_Import', u'count': 240624},\n",
      " {u'_id': u'STBrenden', u'count': 223596},\n",
      " {u'_id': u'Brad Meteor', u'count': 179996},\n",
      " {u'_id': u'Amoebabadass', u'count': 168030},\n",
      " {u'_id': u'Omnific', u'count': 124055}]\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{'$group': {'_id': '$created.user', 'count' : {'$sum': 1}}},\n",
    "            {'$sort': {'count':-1}}, \n",
    "            {\"$limit\":10}]\n",
    "top_ten_users = aggregate(seattle, pipeline)\n",
    "pprint.pprint(top_ten_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of users with only a single post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': 1, u'num_users': 531}]\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{'$group': {'_id': '$created.user', 'count': {'$sum': 1}}},\n",
    "           {'$group': {'_id': '$count', 'num_users': {'$sum':1}}},\n",
    "           {'$sort':{'_id':1}}, \n",
    "           {'$limit':1}]\n",
    "single_poster_count = aggregate(seattle, pipeline)\n",
    "pprint.pprint(single_poster_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postal codes showing corrected US and Canadian formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'V8R 1S1', u'count': 1},\n",
      " {u'_id': u'V8W 2G5', u'count': 1},\n",
      " {u'_id': u'V8W 2A4', u'count': 1},\n",
      " {u'_id': u'V8T 4Y3', u'count': 1},\n",
      " {u'_id': u'V8T 5J8', u'count': 1},\n",
      " {u'_id': u'V9Z 0A1', u'count': 1},\n",
      " {u'_id': u'98253', u'count': 1},\n",
      " {u'_id': u'V8W 1S7', u'count': 1},\n",
      " {u'_id': u'98191', u'count': 1},\n",
      " {u'_id': u'4139', u'count': 1}]\n"
     ]
    }
   ],
   "source": [
    "def make_pipeline():\n",
    "    pipeline = [\n",
    "        {\"$match\": {\"address.postcode\":{\"$exists\":1}}}, \n",
    "        {\"$group\": {\"_id\": \"$address.postcode\", \"count\": {\"$sum\":1}}},\n",
    "        {\"$sort\": {\"count\": 1}}\n",
    "    ]\n",
    "    return pipeline\n",
    "\n",
    "pipeline = make_pipeline()\n",
    "post_code_results = aggregate(seattle, pipeline)\n",
    "pprint.pprint(post_code_results[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### I was confused by the city named \"Capital H Part 1\" \n",
    "I first assumed this was due to users incorrectly assigning places to the Seattle Neighborhood Capitol Hill. After googling around, I realized this is a subsection of the Gulf Islands (near Victoria, B.C.) so I standardized the capitalization. \n",
    "\n",
    "This investigation as well as the high number of Salish names led me to decide not to spell-check city names. Instead this was the singular area I relied upon user input to be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'_id': u'Capital H Part 1', u'count': 739}\n"
     ]
    }
   ],
   "source": [
    "def make_pipeline():\n",
    "    pipeline = [\n",
    "        \n",
    "        {\"$match\": {\"address.city\":{\"$exists\":1}}}, \n",
    "        {\"$group\": {\"_id\": \"$address.city\", \"count\": {\"$sum\":1}}},\n",
    "        {\"$sort\": {\"count\": -1}}\n",
    "    ]\n",
    "    return pipeline\n",
    "\n",
    "pipeline = make_pipeline()\n",
    "city_results = aggregate(db, pipeline)\n",
    "pprint.pprint(city_results[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 20 amenities\n",
    "I'm happy to see the greater Seattle metro area's unique personality is included within this map. We can see users have identified 1/3 the number of bike parking locations as there are car parking locations. As well as more cafe's than bars and pubs combined (many better than Starbucks even)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'parking', u'count': 9014},\n",
      " {u'_id': u'bicycle_parking', u'count': 3083},\n",
      " {u'_id': u'restaurant', u'count': 2838},\n",
      " {u'_id': u'school', u'count': 2614},\n",
      " {u'_id': u'bench', u'count': 1983},\n",
      " {u'_id': u'place_of_worship', u'count': 1561},\n",
      " {u'_id': u'fast_food', u'count': 1352},\n",
      " {u'_id': u'cafe', u'count': 1210},\n",
      " {u'_id': u'fuel', u'count': 1063},\n",
      " {u'_id': u'toilets', u'count': 837},\n",
      " {u'_id': u'bank', u'count': 814},\n",
      " {u'_id': u'waste_basket', u'count': 791},\n",
      " {u'_id': u'bar', u'count': 307},\n",
      " {u'_id': u'pub', u'count': 292},\n",
      " {u'_id': u'post_box', u'count': 290},\n",
      " {u'_id': u'fire_station', u'count': 282},\n",
      " {u'_id': u'drinking_water', u'count': 275},\n",
      " {u'_id': u'pharmacy', u'count': 274},\n",
      " {u'_id': u'shelter', u'count': 249},\n",
      " {u'_id': u'parking_entrance', u'count': 242}]\n"
     ]
    }
   ],
   "source": [
    "def make_pipeline():\n",
    "    pipeline = [\n",
    "        \n",
    "        {\"$match\": {\"amenity\":{\"$exists\":1}}}, \n",
    "        {\"$group\": {\"_id\": \"$amenity\", \"count\": {\"$sum\":1}}},\n",
    "        {\"$sort\": {\"count\": -1}},\n",
    "        {\"$limit\": 20}\n",
    "    ]\n",
    "    return pipeline\n",
    "\n",
    "pipeline = make_pipeline()\n",
    "amenities = aggregate(db, pipeline)\n",
    "pprint.pprint(amenities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Ideas\n",
    "\n",
    "## Incentivizing a larger user/creator network\n",
    "Within the Seattle dataset the top contributors are clearly creating much more data than those outside the top 10 contributers. For example the top contributor has more than 10x the number of contributions as the tenth highest contributor. This disparitiy in the number of contributions each user has submitted, will disincentivize some from becoming new users.\n",
    "\n",
    "OSM could attempt to motivate more contributors by focusing upon sub-areas that are interesting for users. For example if OSM were to put out a challenge for the highest number of contributions improving a specific JSON field/sub-document (any of the amenity sub-categories - for example post your favorite coffee shop). By rotating the focus of these improvement pushes, updating the challenges often and broadcasting these challenges to a wide audience, OSM may be able to bring in more contributions from their less frequent users. I believe if OSM were to institute games of this variety, it may be key to building a more involved user base resulting in improved OSM data, especially if the newest challenge represents a priority for said user. \n",
    "\n",
    "\n",
    "## Improving the Amenities sub-document\n",
    "I feel the amenities field is overly broad without providing a lot of help to users. There are lots of top amenities listed above that likely have more than one amenity that users would be interested in knowing about. If you travel via bicycle it is helpful to know bike parking exists at/near your destination.\n",
    "\n",
    "When I am traveling to a foreign city, a consistent challenge is finding a public restroom while I'm out and exploring all day. Many cafe's, pubs and bars likely have a public restroom as well. If the Amenities field was changed into a sub-document similarly to how the address field was handled, OSM would be able to provide more in-depth applicable information for end-users. Providing more amenity data would enable more relevant querying options, improving end-user experience. By increasing the utility each end-user of OSM gains, OSM will be able to grow the network of their user/creator base. \n",
    "\n",
    "# Conclusion\n",
    "The Seattle data is overly large covering nearly half the State of Washington and a bit of Canada for good measure. Covering international data brought additional challenges for data cleaning, but these were great practice going forward as Data Analysts can never assume the data is correct. \n",
    "\n",
    "The size of the OSM file provided an additional challenge, occasionally I would wait 10-20 minutes for my JSON file to be built. It was fun to see my computer using all 16 GB of RAM while crunching the data though. After dealing with such a large file I understand the importance of running small tests of my data for debugging/code performance. I was happy to not have any issues running the whole dataset once my code was functional. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
